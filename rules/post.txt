After the quiz, think of it like generating **two deliverables** for the host:

1. **Instant “Results Dashboard”** (no AI needed, ready immediately)
2. **AI “Insight Report”** (a structured, easy-to-read narrative with charts + recommendations)

If you do it this way, the host always gets something right away, and the AI report feels like a bonus that matures a minute later.

---

# 1) Post-quiz deliverables (what the host sees)

## A) Instant Dashboard (generated from Redis snapshots)

Sections:

* **Leaderboard** (final)
* **Completion + friction**

  * completion rate
  * skip rate overall
  * most-skipped questions
* **Per-question breakdown**

  * rating histograms + mean/median
  * sat/unsat/skip stats (for essay follow-ups)
  * top themes + top missing details
  * “common misunderstandings”
* **Room summary**

  * top themes across room
  * top contrasts (axes of disagreement)
  * “most interesting outliers” (small list)

This is deterministic and fast.

## B) AI Insight Report (generated by a post-room job)

A clean narrative that reads like:

* “Here’s what people think”
* “Here’s where they disagree”
* “Here’s what to ask next”
* “Here’s how to improve your survey”

This is where you use Gemini heavily, but only once.

---

# 2) How to generate the Instant Dashboard (no AI)

### Step 1: Freeze snapshot on room end

When room ends:

* read Redis keys:

  * `room:{code}:lb`
  * `room:{code}:q:{Qk}:profile` for all questions
  * `room:{code}:memory`
  * player counts / completion stats

Write a **RoomSnapshot** document to Mongo:

* `roomCode, endedAt`
* final leaderboard top N
* per-question profiles
* room memory summary

Now your frontend has a stable “report ID” to fetch.

### Step 2: Render it nicely

The server can return JSON; frontend renders:

* bar charts for rating histograms
* cards for “top themes”
* heatmap-style rows for skip/unsat

No need for PDFs at first; just a good dashboard page.

---

# 3) How to generate the AI Insight Report (easy-to-read)

### The trick: don’t feed Gemini raw data dumps

You build a **Report Input Pack** first.

## Report Input Pack (curated, small)

Contains:

* scope anchor (host intent + presentation summary)
* per-question profiles (already compact)
* top clusters/contrasts from room memory
* **sampled evidence snippets** (very important, but limited)

### Evidence sampling strategy (crucial)

You never send all answers. You sample:

* per question:

  * 3–5 representative 1-sentence summaries from top themes
  * 1–2 outlier summaries
* per contrast/cluster:

  * 2 examples per side

This gives Gemini “proof” without token explosion.

---

## AI report output should be structured (JSON)

Generate a report object like:

* `executiveSummary` (5 bullets)
* `keyThemes`:

  * theme name
  * what it means
  * % of participants
  * evidence snippets (short)
* `contrasts`:

  * axis name
  * side A vs side B
  * what predicts each side (if possible)
* `perQuestionInsights`:

  * what worked
  * misunderstandings
  * what people wanted but didn’t explain
  * best follow-ups (that worked)
* `frictionAnalysis`:

  * where people skipped
  * hypothesized reason
* `recommendedNextQuestions` (for next survey)
* `recommendedEditsToExistingQuestions`

Then the UI renders it as a clean report page.

---

# 4) “Easy-to-read” UI layout (what actually looks good)

### Page 1: “At a glance”

* **Top 5 findings** (bullets)
* **2–3 key charts**

  * overall completion
  * biggest pain point rating chart
  * theme distribution

### Page 2: Themes

* cards per theme:

  * label + % + explanation
  * 2–3 anonymized example snippets

### Page 3: Contrasts / segments

* “People split into 3 groups”
* small table:

  * group label
  * size
  * defining traits
  * recommended questions to probe them next time

### Page 4: Per question breakdown

* each question shows:

  * distribution
  * misunderstandings
  * best probes
  * friction stats

### Page 5: “What to do next”

* recommended new questions
* recommended edits
* recommended follow-up modes

This is the type of report founders/product teams love.

---

# 5) How it links back to generating new questions

The report should explicitly output:

## A) Question improvements (rewrite suggestions)

Based on:

* misunderstandings
* high skip/unsat rate
* high variance + unclear phrasing

Example:

* “Q3 is interpreted in 2 ways; split into Q3a/Q3b”

## B) New base questions to add

Based on:

* dominant themes that weren’t directly asked
* big contrasts that need clearer measurement
* gaps detected from missing-details patterns

## C) Branch modules (advanced but gold)

Generate “branch sets”:

* If user is in cluster A → ask these 2 extra questions
* If rating ≤2 → ask these 2 probes
* If inconsistent → ask this challenge question

This becomes your next iteration of the survey template.

---

# 6) Host access after the quiz (product flow)

Host after ending room:

* sees “Generating report…” for AI insights
* immediately can view:

  * snapshot dashboard
* when AI report is ready:

  * toast notification or auto-refresh
  * “Download / Export” options

Endpoints:

* `GET /v1/reports/{roomCode}/snapshot` (instant)
* `GET /v1/reports/{roomCode}/ai` (ready when done; else status=pending)
* `GET /v1/reports/{roomCode}/export.csv` (optional)
* `GET /v1/reports/{roomCode}/export.json`

---

# 7) One extra feature that makes it feel premium

**Comparisons** (host loves this):

* compare this room vs previous room (same survey)
* show deltas:

  * ratings changed
  * themes shifted
  * friction improved/worsened

That’s Level 5-ish, but you can do it lightly later.

---

If you want, I can draft the exact JSON shapes for:

* `RoomSnapshot`
* `AIReport`
* and the “evidence sampling” pipeline (how to pick snippets)
